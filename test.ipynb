{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = '1920X1080' # possible resolutions: '800X600' '640X480' '320X240' '160X120' '1920X1080'\n",
    "RENDER = True       # If render is true, resolution is always 1920x1080 to match my screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working examples (random agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.game_variable_buffer to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.game_variable_buffer` for environment variables or `env.get_wrapper_attr('game_variable_buffer')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "11\n",
      "6\n",
      "0\n",
      "5\n",
      "2\n",
      "5\n",
      "7\n",
      "0\n",
      "6\n",
      "4\n",
      "1\n",
      "8\n",
      "9\n",
      "1\n",
      "5\n",
      "7\n",
      "7\n",
      "11\n",
      "7\n",
      "6\n",
      "9\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11\n",
      "11\n",
      "0\n",
      "6\n",
      "11\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "11\n",
      "8\n",
      "10\n",
      "5\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "0\n",
      "10\n",
      "1\n",
      "2\n",
      "10\n",
      "5\n",
      "3\n",
      "2\n",
      "8\n",
      "3\n",
      "4\n",
      "8\n",
      "9\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "4\n",
      "4\n",
      "10\n",
      "5\n",
      "1\n",
      "6\n",
      "10\n",
      "9\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "8\n",
      "11\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "9\n",
      "5\n",
      "4\n",
      "5\n",
      "0\n",
      "8\n",
      "10\n",
      "2\n",
      "7\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "11\n",
      "10\n",
      "11\n",
      "8\n",
      "6\n",
      "11\n",
      "9\n",
      "8\n",
      "4\n",
      "7\n",
      "11\n",
      "11\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n",
      "10\n",
      "5\n",
      "6\n",
      "3\n",
      "9\n",
      "5\n",
      "8\n",
      "1\n",
      "1\n",
      "9\n",
      "8\n",
      "8\n",
      "8\n",
      "4\n",
      "8\n",
      "5\n",
      "0\n",
      "5\n",
      "11\n",
      "10\n",
      "4\n",
      "9\n",
      "3\n",
      "7\n",
      "8\n",
      "0\n",
      "1\n",
      "8\n",
      "10\n",
      "3\n",
      "3\n",
      "8\n",
      "7\n",
      "10\n",
      "10\n",
      "2\n",
      "4\n",
      "10\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "7\n",
      "2\n",
      "8\n",
      "4\n",
      "11\n",
      "4\n",
      "10\n",
      "11\n",
      "8\n",
      "6\n",
      "9\n",
      "0\n",
      "2\n",
      "8\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "8\n",
      "7\n",
      "2\n",
      "2\n",
      "2\n",
      "7\n",
      "8\n",
      "6\n",
      "8\n",
      "10\n",
      "1\n",
      "8\n",
      "7\n",
      "7\n",
      "6\n",
      "6\n",
      "3\n",
      "0\n",
      "10\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "10\n",
      "2\n",
      "10\n",
      "4\n",
      "11\n",
      "10\n",
      "5\n",
      "8\n",
      "3\n",
      "7\n",
      "9\n",
      "10\n",
      "7\n",
      "10\n",
      "9\n",
      "5\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "2\n",
      "8\n",
      "8\n",
      "7\n",
      "4\n",
      "4\n",
      "8\n",
      "7\n",
      "1\n",
      "9\n",
      "1\n",
      "8\n",
      "8\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "10\n",
      "5\n",
      "3\n",
      "8\n",
      "8\n",
      "8\n",
      "3\n",
      "1\n",
      "5\n",
      "9\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "7\n",
      "0\n",
      "3\n",
      "8\n",
      "2\n",
      "1\n",
      "3\n",
      "5\n",
      "0\n",
      "8\n",
      "1\n",
      "1\n",
      "8\n",
      "11\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "10\n",
      "4\n",
      "10\n",
      "4\n",
      "5\n",
      "0\n",
      "5\n",
      "9\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "7\n",
      "10\n",
      "6\n",
      "3\n",
      "3\n",
      "10\n",
      "4\n",
      "6\n",
      "10\n",
      "6\n",
      "7\n",
      "0\n",
      "11\n",
      "7\n",
      "11\n",
      "3\n",
      "3\n",
      "8\n",
      "8\n",
      "11\n",
      "5\n",
      "11\n",
      "4\n",
      "6\n",
      "8\n",
      "3\n",
      "2\n",
      "8\n",
      "7\n",
      "1\n",
      "6\n",
      "11\n",
      "1\n",
      "11\n",
      "4\n",
      "7\n",
      "5\n",
      "3\n",
      "10\n",
      "1\n",
      "0\n",
      "10\n",
      "6\n",
      "2\n",
      "4\n",
      "0\n",
      "9\n",
      "3\n",
      "6\n",
      "0\n",
      "4\n",
      "8\n",
      "1\n",
      "10\n",
      "10\n",
      "1\n",
      "4\n",
      "4\n",
      "1\n",
      "11\n",
      "11\n",
      "9\n",
      "0\n",
      "6\n",
      "4\n",
      "1\n",
      "1\n",
      "8\n",
      "5\n",
      "11\n",
      "10\n",
      "0\n",
      "3\n",
      "11\n",
      "11\n",
      "2\n",
      "8\n",
      "0\n",
      "5\n",
      "10\n",
      "11\n",
      "4\n",
      "3\n",
      "10\n",
      "10\n",
      "11\n",
      "10\n",
      "1\n",
      "7\n",
      "0\n",
      "6\n",
      "8\n",
      "1\n",
      "4\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "8\n",
      "3\n",
      "9\n",
      "11\n",
      "8\n",
      "9\n",
      "7\n",
      "9\n",
      "11\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "8\n",
      "10\n",
      "3\n",
      "0\n",
      "2\n",
      "11\n",
      "5\n",
      "0\n",
      "11\n",
      "8\n",
      "6\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "10\n",
      "11\n",
      "11\n",
      "6\n",
      "7\n",
      "6\n",
      "3\n",
      "10\n",
      "9\n",
      "9\n",
      "6\n",
      "9\n",
      "3\n",
      "7\n",
      "3\n",
      "5\n",
      "1\n",
      "0\n",
      "7\n",
      "5\n",
      "2\n",
      "0\n",
      "8\n",
      "4\n",
      "8\n",
      "11\n",
      "2\n",
      "11\n",
      "11\n",
      "2\n",
      "11\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Sample random action\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m---> 16\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     18\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\wrappers\\frame_stack.py:179\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\wrappers\\normalize.py:81\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env:\n\u001b[0;32m     83\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(obs)\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\COOM\\wrappers\\observation.py:23\u001b[0m, in \u001b[0;36mRescale.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m---> 23\u001b[0m     state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, reward, done, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\COOM\\wrappers\\observation.py:43\u001b[0m, in \u001b[0;36mResize.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m---> 43\u001b[0m     state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mresize(state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), reward, done, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\COOM\\env\\scenario.py:189\u001b[0m, in \u001b[0;36mDoomEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    187\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_actions[action]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mset_action(action)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m    192\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SINGLE ENVIRONMENT\n",
    "\n",
    "from COOM.env.builder import make_env\n",
    "from COOM.utils.config import Scenario\n",
    "\n",
    "\n",
    "env = make_env(scenario=Scenario.FLOOR_IS_LAVA, resolution=RESOLUTION, render=RENDER)\n",
    "\n",
    "# Initialize and use the environment\n",
    "env.reset()\n",
    "for steps in range(1000):\n",
    "    action = env.action_space.sample()  # Sample random action\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COOM.env.continual import ContinualLearningEnv\n",
    "from COOM.utils.config import Sequence \n",
    "\n",
    "'''\n",
    "\n",
    "cl_env = ContinualLearningEnv(Sequence.CO8)\n",
    "for env in cl_env.tasks:\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "EPISODES = 1\n",
    "STEPS_PER_EPISODE = 100\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "12\n",
      "28224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Desktop\\coom-continual-learning-on-a-doom-benchmark\\soft_AC.py:120: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  state = torch.FloatTensor(state).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x84672 and 28224x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCOOM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCOOM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scenario\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoft_AC\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SACAgent, ReplayBuffer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize and use the environment\u001b[39;00m\n\u001b[0;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(scenario\u001b[38;5;241m=\u001b[39mScenario\u001b[38;5;241m.\u001b[39mFLOOR_IS_LAVA, resolution\u001b[38;5;241m=\u001b[39mRESOLUTION, render\u001b[38;5;241m=\u001b[39mRENDER)\n",
      "File \u001b[1;32mx:\\Desktop\\coom-continual-learning-on-a-doom-benchmark\\soft_AC.py:267\u001b[0m\n\u001b[0;32m    265\u001b[0m losses_ep \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m--> 267\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     next_state, reward, terminated, truncated, _\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m#if truncated: print(\"!!! truncated\")\u001b[39;00m\n",
      "File \u001b[1;32mx:\\Desktop\\coom-continual-learning-on-a-doom-benchmark\\soft_AC.py:121\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    120\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 121\u001b[0m     mean, log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     std \u001b[38;5;241m=\u001b[39m log_std\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m    124\u001b[0m     normal \u001b[38;5;241m=\u001b[39m Normal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mx:\\Desktop\\coom-continual-learning-on-a-doom-benchmark\\soft_AC.py:99\u001b[0m, in \u001b[0;36mAgent.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 99\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[0;32m    102\u001b[0m     mean    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_linear(x)\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\damic\\anaconda3\\envs\\rl_env2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x84672 and 28224x256)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# SINGLE ENVIRONMENT\n",
    "\n",
    "from COOM.env.builder import make_env\n",
    "from COOM.utils.config import Scenario\n",
    "\n",
    "from soft_AC import SACAgent, ReplayBuffer\n",
    "'''\n",
    "# Initialize and use the environment\n",
    "env = make_env(scenario=Scenario.FLOOR_IS_LAVA, resolution=RESOLUTION, render=RENDER)\n",
    "print(env.observation_space.shape)\n",
    "[print(a) for a in env.available_actions]\n",
    "print(env.action_space.n)\n",
    "\n",
    "# Training loop\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "#max_action = env.action_space.high[0]\n",
    "\n",
    "agent = SACAgent(state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(STEPS_PER_EPISODE):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, next_state, reward, float(done)))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if replay_buffer.size() > BATCH_SIZE:\n",
    "            agent.train(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
